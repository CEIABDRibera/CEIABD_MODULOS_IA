{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HUGGING FACE - INTRODUCCIÓN   \n",
    "\n",
    "<img src=\"./img/huggingfacelogo.svg\">\n",
    "\n",
    "[Hugging Face](https://huggingface.co/) se fundó en 2016 con el objetivo de desarrollar una aplicación de chabot. A partir de 2018 comenzó a abrir a la comunidad open source partes de sus modelos NLP, los cuales ganaron popularidad dentro de la comunidad IA.\n",
    "\n",
    "Gran parte de la fama adquirida por Hugging Face se debe a sus librerías, en concreto [Transformers](https://huggingface.co/docs/transformers/index) (para tratamiento de textos) y [Diffusers](https://huggingface.co/docs/diffusers/index) (para tareas de imagen), facilitando a los desarrolladores importar y trabajar con una gran variedad de modelos pre-entrenados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traductor inglés-castellano usando la librería `Transformers`   \n",
    "\n",
    "Usando la librería que nos proporciona Hugging Face para trabajar con modelos tipo `Transformers`, en pocas líneas de código se consigue cargar un modelo y crear un *pipeline* en el que se indica que el modelo en cuestión es para una tarea de traducción.    \n",
    "\n",
    "Un *pipeline* es un 'elemento' que permite simplificar el código sin necesidad de gestionar detalles del modelo a usar.   \n",
    "\n",
    "Creado el *pipeline*, este se invoca pasándole una frase como parámetro de entrada al modelo que, después de realizar la inferencia, este devuelve la frase traducida como salida.   \n",
    "\n",
    "Veamos el código en cuestión:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-03-15 17:23:49.350448: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-03-15 17:23:49.356023: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-03-15 17:23:49.372137: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1742059429.394115   20736 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1742059429.400244   20736 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1742059429.416981   20736 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742059429.417023   20736 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742059429.417027   20736 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742059429.417029   20736 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-03-15 17:23:49.422985: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# pip install transformers\n",
    "# pip install tf-keras \n",
    "# pip uninstall keras (para evitar conflictos con tf-keras)\n",
    "# pip install sentencepiece\n",
    "# pip install sacremoses\n",
    "from transformers import pipeline\n",
    "# Frase original en inglés\n",
    "frase_en = \"I like to play basketball and being a geek\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Pipeline*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-15 17:24:20.955725: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n",
      "All model checkpoint layers were used when initializing TFMarianMTModel.\n",
      "\n",
      "All the layers of TFMarianMTModel were initialized from the model checkpoint at Helsinki-NLP/opus-mt-en-es.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMarianMTModel for predictions without further training.\n",
      "/usr/local/lib/python3.11/site-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
      "Device set to use 0\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-es\")\n",
    "frase_es = pipe(frase_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traducción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traducción:  Me gusta jugar al baloncesto y ser un friki\n"
     ]
    }
   ],
   "source": [
    "# Salida de la traducción\n",
    "print(\"Traducción: \", frase_es[0]['translation_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generación de imágenes con `Diffusers`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso, en lugar de la librería `transformers` , necesitamos utilizar la librería `diffusers` para generar datos similares a los datos con los que se ha entrenado.\n",
    "\n",
    "Para ello se utiliza el modelo `Stable Diffusion` (una librería estándar de generación de imágenes), y, nuevamente, con unas pocas líneas de código es posible generar una imagen a petición del consumidor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install diffusers\n",
    "#pip install torch\n",
    "from diffusers import DiffusionPipeline\n",
    "import torch\n",
    "\n",
    "pipeline = DiffusionPipeline.from_pretrained(\"sd-legacy/stable-diffusion-v1-5\", torch_dtype=torch.float16)\n",
    "# pipeline.to(\"cuda\") # Para usar GPU\n",
    "pipeline.to(\"cpu\") # Para usar CPU\n",
    "pipeline(\"Una imagen de Batman sin máscara\").images[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***NOTA***: Dado que este apartado requiere mucho tiempo y mucha capacidad de CPU/GPU se sube este notebook a COLAB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
