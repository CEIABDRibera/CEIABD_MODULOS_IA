{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_text as tf_text\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hiperparámetros\n",
    "BUFFER_SIZE = 20000\n",
    "BATCH_SIZE = 64\n",
    "MAX_TOKENS = 128\n",
    "EMBEDDING_DIM = 512\n",
    "NUM_HEADS = 8\n",
    "FF_DIM = 2048\n",
    "NUM_LAYERS = 6\n",
    "DROPOUT_RATE = 0.1\n",
    "VOCAB_SIZE = 10000  # Ajustar según el tamaño real del vocabulario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Carga y Preprocesamiento del Dataset\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = tf.strings.lower(sentence)\n",
    "    sentence = tf.strings.regex_replace(sentence, r\"[^a-zñáéíóúüç\\s]\", '')\n",
    "    sentence = tf.strings.strip(sentence)\n",
    "    sentence = tf.concat(['<start>', sentence, '<end>'], axis=-1)\n",
    "    return sentence\n",
    "\n",
    "def load_and_preprocess_data(max_tokens=MAX_TOKENS, vocab_size=VOCAB_SIZE):\n",
    "    examples, metadata = tfds.load('ted_hrlr_translate/en_es_conversational', with_info=True,\n",
    "                                   as_supervised=True)\n",
    "    train_examples, val_examples = examples['train'], examples['validation']\n",
    "\n",
    "    def filter_max_tokens(en, es):\n",
    "        return tf.logical_and(tf.size(en) <= max_tokens, tf.size(es) <= max_tokens)\n",
    "\n",
    "    train_batches = train_examples.map(lambda en, es: (preprocess_sentence(en), preprocess_sentence(es)))\n",
    "    val_batches = val_examples.map(lambda en, es: (preprocess_sentence(en), preprocess_sentence(es)))\n",
    "\n",
    "    tokenizer_en = tf_text.UnicodeTextTokenizer()\n",
    "    tokenizer_es = tf_text.UnicodeTextTokenizer()\n",
    "\n",
    "    def tokenize_pairs(en, es):\n",
    "        return tokenizer_en.tokenize(en).to_tensor(), tokenizer_es.tokenize(es).to_tensor()\n",
    "\n",
    "    train_batches = train_batches.map(tokenize_pairs).filter(filter_max_tokens).cache().shuffle(BUFFER_SIZE).padded_batch(BATCH_SIZE)\n",
    "    val_batches = val_batches.map(tokenize_pairs).filter(filter_max_tokens).padded_batch(BATCH_SIZE)\n",
    "\n",
    "    def build_vocabulary(dataset, tokenizer, vocab_size):\n",
    "        vocabulary_set = set()\n",
    "        for en_tokens, es_tokens in dataset:\n",
    "            for tokens in [en_tokens, es_tokens]:\n",
    "                for token_list in tokens.numpy():\n",
    "                    for token in token_list:\n",
    "                        if token!= 0:  # Skip padding tokens\n",
    "                            vocabulary_set.add(token)\n",
    "        vocabulary_list = [b'<pad>'] + sorted(list(vocabulary_set))\n",
    "        if len(vocabulary_list) > vocab_size:\n",
    "            vocabulary_list = vocabulary_list[:vocab_size]\n",
    "        return tf.constant(vocabulary_list)\n",
    "\n",
    "    vocabulary_en = build_vocabulary(train_batches, tokenizer_en, vocab_size)\n",
    "    vocabulary_es = build_vocabulary(train_batches, tokenizer_es, vocab_size)\n",
    "\n",
    "    index_from_string_en = tf.keras.layers.StringLookup(vocabulary=vocabulary_en, mask_token='')\n",
    "    string_from_index_en = tf.keras.layers.StringLookup(vocabulary=vocabulary_en, invert=True, mask_token='')\n",
    "\n",
    "    index_from_string_es = tf.keras.layers.StringLookup(vocabulary=vocabulary_es, mask_token='')\n",
    "    string_from_index_es = tf.keras.layers.StringLookup(vocabulary=vocabulary_es, invert=True, mask_token='')\n",
    "\n",
    "    return train_batches, val_batches, index_from_string_en, string_from_index_en, index_from_string_es, string_from_index_es\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches, val_batches, index_from_string_en, string_from_index_en, index_from_string_es, string_from_index_es = load_and_preprocess_data(MAX_TOKENS, VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuraciones disponibles para ted_hrlr_translate:\n",
      "- Nombre: az_to_en\n",
      "  Versión: 1.0.0\n",
      "  Descripción: Translation dataset from az to en in plain text.\n",
      "\n",
      "- Nombre: aztr_to_en\n",
      "  Versión: 1.0.0\n",
      "  Descripción: Translation dataset from az_tr to en in plain text.\n",
      "\n",
      "- Nombre: be_to_en\n",
      "  Versión: 1.0.0\n",
      "  Descripción: Translation dataset from be to en in plain text.\n",
      "\n",
      "- Nombre: beru_to_en\n",
      "  Versión: 1.0.0\n",
      "  Descripción: Translation dataset from be_ru to en in plain text.\n",
      "\n",
      "- Nombre: es_to_pt\n",
      "  Versión: 1.0.0\n",
      "  Descripción: Translation dataset from es to pt in plain text.\n",
      "\n",
      "- Nombre: fr_to_pt\n",
      "  Versión: 1.0.0\n",
      "  Descripción: Translation dataset from fr to pt in plain text.\n",
      "\n",
      "- Nombre: gl_to_en\n",
      "  Versión: 1.0.0\n",
      "  Descripción: Translation dataset from gl to en in plain text.\n",
      "\n",
      "- Nombre: glpt_to_en\n",
      "  Versión: 1.0.0\n",
      "  Descripción: Translation dataset from gl_pt to en in plain text.\n",
      "\n",
      "- Nombre: he_to_pt\n",
      "  Versión: 1.0.0\n",
      "  Descripción: Translation dataset from he to pt in plain text.\n",
      "\n",
      "- Nombre: it_to_pt\n",
      "  Versión: 1.0.0\n",
      "  Descripción: Translation dataset from it to pt in plain text.\n",
      "\n",
      "- Nombre: pt_to_en\n",
      "  Versión: 1.0.0\n",
      "  Descripción: Translation dataset from pt to en in plain text.\n",
      "\n",
      "- Nombre: ru_to_en\n",
      "  Versión: 1.0.0\n",
      "  Descripción: Translation dataset from ru to en in plain text.\n",
      "\n",
      "- Nombre: ru_to_pt\n",
      "  Versión: 1.0.0\n",
      "  Descripción: Translation dataset from ru to pt in plain text.\n",
      "\n",
      "- Nombre: tr_to_en\n",
      "  Versión: 1.0.0\n",
      "  Descripción: Translation dataset from tr to en in plain text.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# Obtén el constructor del dataset\n",
    "builder = tfds.builder(\"ted_hrlr_translate\")\n",
    "\n",
    "# Imprime las configuraciones disponibles y sus descripciones/versiones\n",
    "print(\"Configuraciones disponibles para ted_hrlr_translate:\")\n",
    "for config_name, config_obj in builder.builder_configs.items():\n",
    "    print(f\"- Nombre: {config_name}\")\n",
    "    print(f\"  Versión: {config_obj.version}\")\n",
    "    print(f\"  Descripción: {config_obj.description}\\n\")\n",
    "\n",
    "# También puedes simplemente listar los nombres:\n",
    "# print(builder.builder_configs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2. Implementación de los Componentes del Transformer\n",
    "\n",
    "# 2.1. Embedding y Codificación Posicional\n",
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__()\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=True)\n",
    "        self.pos_encoding = self.positional_encoding(MAX_TOKENS, d_model)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'vocab_size': self.embedding.input_dim,\n",
    "            'd_model': self.embedding.output_dim,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def positional_encoding(self, position, d_model):\n",
    "        angle_rads = np.arange(position)[:, np.newaxis] / np.power(10000, (2 * (np.arange(d_model)[np.newaxis, :] // 2)) / np.float32(d_model))\n",
    "        # apply sin to even indices in the array; 2i\n",
    "        angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "        # apply cos to odd indices in the array; 2i+1\n",
    "        angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "        pos_encoding = tf.cast(angle_rads[np.newaxis,...], dtype=tf.float32)\n",
    "        return pos_encoding\n",
    "\n",
    "    def compute_mask(self, *args, **kwargs):\n",
    "        return self.embedding.compute_mask(*args, **kwargs)\n",
    "\n",
    "    def call(self, x):\n",
    "        length = tf.shape(x)[1]\n",
    "        x = self.embedding(x)\n",
    "        # This factor sets the relative scale of the embedding and positonal_encoding.\n",
    "        x *= tf.math.sqrt(tf.cast(self.embedding.output_dim, tf.float32))\n",
    "        x = x + self.pos_encoding[:, :length, :]\n",
    "        return x\n",
    "\n",
    "# 2.2. Mecanismo de Atención Multi-Cabeza\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'num_heads': self.num_heads,\n",
    "            'd_model': self.d_model,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def scaled_dot_product_attention(self, q, k, v, mask):\n",
    "        matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "        # scale matmul_qk\n",
    "        dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "        scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "        # add the mask to the scaled tensor.\n",
    "        if mask is not None:\n",
    "            scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "        # softmax is normalized on the last axis (seq_len_k so that the scores\n",
    "        # add up to 1).\n",
    "        attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "        output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "        return output, attention_weights\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"Split the last dimension into (num_heads, depth).\n",
    "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "        \"\"\"\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=)\n",
    "\n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)\n",
    "\n",
    "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "\n",
    "        scaled_attention, attention_weights = self.scaled_dot_product_attention(\n",
    "            q, k, v, mask)\n",
    "\n",
    "        scaled_attention = tf.transpose(scaled_attention,\n",
    "                                        perm=)  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "        concat_attention = tf.reshape(scaled_attention,\n",
    "                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        return output, attention_weights\n",
    "\n",
    "# 2.3. Red Neuronal Feed-Forward Punto a Punto\n",
    "def point_wise_feed_forward_network(d_model, ff_dim):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(ff_dim, activation='relu'),  # (batch_size, seq_len, ff_dim)\n",
    "        tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "    ])\n",
    "\n",
    "# 2.4. Bloque del Codificador\n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, ff_dim)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'd_model': self.mha.d_model,\n",
    "            'num_heads': self.mha.num_heads,\n",
    "            'ff_dim': self.ffn.layers.units,\n",
    "            'rate': self.dropout1.rate,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "        attn_output, _ = self.mha(x, x, x, mask)  # Self attention (q, k, v are the same input)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)  # Residual connection and layer norm\n",
    "\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # Residual connection and layer norm\n",
    "\n",
    "        return out2\n",
    "\n",
    "# 2.5. Bloque del Decodificador\n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, ff_dim)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'd_model': self.mha1.d_model,\n",
    "            'num_heads': self.mha1.num_heads,\n",
    "            'ff_dim': self.ffn.layers.units,\n",
    "            'rate': self.dropout1.rate,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
    "        # Masked self attention\n",
    "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(x + attn1)\n",
    "\n",
    "        # Encoder-decoder attention\n",
    "        attn2, attn_weights_block2 = self.mha2(\n",
    "            enc_output, enc_output, out1, padding_mask)  # value, key, query\n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        out2 = self.layernorm2(out1 + attn2)\n",
    "\n",
    "        # Feed forward network\n",
    "        ffn_output = self.ffn(out2)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(out2 + ffn_output)\n",
    "\n",
    "        return out3, attn_weights_block1, attn_weights_block2\n",
    "\n",
    "# 2.6. El Codificador\n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, ff_dim, input_vocab_size, rate=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = PositionalEmbedding(input_vocab_size, d_model)\n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, ff_dim, rate)\n",
    "                           for _ in range(num_layers)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'num_layers': self.num_layers,\n",
    "            'd_model': self.d_model,\n",
    "            'num_heads': self.enc_layers.mha.num_heads,\n",
    "            'ff_dim': self.enc_layers.ffn.layers.units,\n",
    "            'input_vocab_size': self.embedding.embedding.input_dim,\n",
    "            'rate': self.dropout.rate,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "\n",
    "        # adding embedding and position encoding.\n",
    "        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training, mask)\n",
    "\n",
    "        return x  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "# 2.7. El Decodificador\n",
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, ff_dim, target_vocab_size, rate=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = PositionalEmbedding(target_vocab_size, d_model)\n",
    "        self.dec_layers = [DecoderLayer(d_model, num_heads, ff_dim, rate)\n",
    "                           for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'num_layers': self.num_layers,\n",
    "            'd_model': self.d_model,\n",
    "            'num_heads': self.dec_layers.mha1.num_heads,\n",
    "            'ff_dim': self.dec_layers.ffn.layers.units,\n",
    "            'target_vocab_size': self.embedding.embedding.input_dim,\n",
    "            'rate': self.dropout.rate,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "\n",
    "        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
    "                                                 look_ahead_mask, padding_mask)\n",
    "\n",
    "            attention_weights[f'decoder_layer{i+1}_block1'] = block1\n",
    "            attention_weights[f'decoder_layer{i+1}_block2'] = block2\n",
    "\n",
    "        return x, attention_weights\n",
    "\n",
    "# 2.8. El Transformer\n",
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, ff_dim, input_vocab_size,\n",
    "                 target_vocab_size, rate=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = Encoder(num_layers, d_model, num_heads, ff_dim,\n",
    "                               input_vocab_size, rate)\n",
    "\n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, ff_dim,\n",
    "                               target_vocab_size, rate)\n",
    "\n",
    "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'num_layers': self.encoder.num_layers,\n",
    "            'd_model': self.encoder.d_model,\n",
    "            'num_heads': self.encoder.enc_layers.mha.num_heads,\n",
    "            'ff_dim': self.encoder.enc_layers.ffn.layers.units,\n",
    "            'input_vocab_size': self.encoder.embedding.embedding.input_dim,\n",
    "            'target_vocab_size': self.decoder.embedding.embedding.input_dim,\n",
    "            'rate': self.encoder.dropout.rate,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def create_padding_mask(self, seq):\n",
    "        seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "        # add extra dimensions so that padding can be added to the attention\n",
    "        # logits.\n",
    "        return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
    "\n",
    "    def create_look_ahead_mask(self, size):\n",
    "        mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "        return mask  # (seq_len, seq_len)\n",
    "\n",
    "    def call(self, inp, tar, training):\n",
    "        enc_padding_mask = self.create_padding_mask(inp)\n",
    "        dec_padding_mask = self.create_padding_mask(tar)\n",
    "\n",
    "        # look_ahead mask is used to mask the future tokens in the decoder.\n",
    "        look_ahead_mask = self.create_look_ahead_mask(tf.shape(tar)[1])\n",
    "        dec_target_padding_mask = self.create_padding_mask(tar)\n",
    "        combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "\n",
    "        enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
    "\n",
    "        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
    "        dec_output, attention_weights = self.decoder(\n",
    "            tar, enc_output, training, combined_mask, enc_padding_mask)\n",
    "\n",
    "        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
    "\n",
    "        return final_output, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Creación del Modelo\n",
    "transformer = Transformer(\n",
    "    num_layers=NUM_LAYERS,\n",
    "    d_model=EMBEDDING_DIM,\n",
    "    num_heads=NUM_HEADS,\n",
    "    ff_dim=FF_DIM,\n",
    "    input_vocab_size=VOCAB_SIZE,\n",
    "    target_vocab_size=VOCAB_SIZE,\n",
    "    rate=DROPOUT_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Entrenamiento del Modelo\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def masked_loss(y_true, y_pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(y_true, 0))\n",
    "    loss_ = loss_fn(y_true, y_pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)\n",
    "\n",
    "train_accuracy_metric = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "@tf.function\n",
    "def train_step(inp, tar):\n",
    "    tar_inp = tar[:, :-1]\n",
    "    tar_real = tar[:, 1:]\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, _ = transformer(inp, tar_inp, training=True)\n",
    "        loss = masked_loss(tar_real, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, transformer.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "\n",
    "    train_accuracy_metric.update_state(tar_real, predictions)\n",
    "    return loss\n",
    "\n",
    "EPOCHS = 20 # Puedes ajustar el número de épocas\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f'Epoch {epoch + 1}')\n",
    "    total_loss = 0\n",
    "    train_accuracy_metric.reset_state()\n",
    "    for (batch, (inp, tar)) in enumerate(train_batches):\n",
    "        loss = train_step(inp, tar)\n",
    "        total_loss += loss\n",
    "        if batch % 100 == 0:\n",
    "            print(f'Batch {batch} Loss {loss.numpy():.4f} Accuracy {train_accuracy_metric.result().numpy():.4f}')\n",
    "    print(f'Epoch {epoch + 1} Loss {total_loss / (batch + 1):.4f} Accuracy {train_accuracy_metric.result().numpy():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 5. Realización de la Traducción (Inferencia)\n",
    "def translate(sentence, index_from_string_en, string_from_index_es, index_from_string_es, transformer):\n",
    "    sentence = preprocess_sentence(tf.constant(sentence)).numpy().decode('utf-8')\n",
    "    input_tokens = index_from_string_en(tf.constant([sentence]))\n",
    "    input_tokens = input_tokens\n",
    "\n",
    "    decoder_input = [index_from_string_es('<start>').numpy()]\n",
    "    output = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)\n",
    "    output = output.write(0, tf.constant(decoder_input))\n",
    "\n",
    "    for i in tf.range(MAX_TOKENS):\n",
    "        predictions, _ = transformer(input_tokens, output.stack(), training=False)\n",
    "        # select the last word from the seq_len dimension\n",
    "        predictions = predictions[:, -1, :]  # (batch_size, 1, vocab_size)\n",
    "        predicted_id = tf.argmax(predictions, axis=-1).numpy()\n",
    "\n",
    "        if predicted_id == index_from_string_es('<end>').numpy():\n",
    "            break\n",
    "\n",
    "        decoder_input.append(predicted_id)\n",
    "        output = output.write(i + 1, tf.constant(decoder_input))\n",
    "\n",
    "    translated_tokens = output.stack().numpy()\n",
    "    translated_sentence = string_from_index_es(translated_tokens).numpy()\n",
    "    translated_sentence = translated_sentence[1:-1]  # Remove '<start>' and '<end>'\n",
    "    return translated_sentence.decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ejemplo de traducción\n",
    "example_input = \"Hello, how are you?\"\n",
    "translated_output = translate(example_input, index_from_string_en, string_from_index_es, index_from_string_es, transformer)\n",
    "print(f\"Input: {example_input}\")\n",
    "print(f\"Translation: {translated_output}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
